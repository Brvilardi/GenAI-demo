{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AJ7MPCLGKCx"
   },
   "source": [
    "# **Resumo**\n",
    "\n",
    "A ideia do Projeto é utilizar técnicas de Inteligência Artificial com grandes modelos de linguagem (LLM) para construir um sistema de perguntas e respostas usando uma base de conhecimento de documentação de serviços da AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3PcvmiDGY0l"
   },
   "source": [
    "# **Descrição do Problema**\n",
    "\n",
    "Os LLMs são modelos que só tem conhecimento do que foi usado para os treinar. As documentações da AWS são atualizadas com muita frequência, por isso, usar LLMs para perguntas sobre serviços AWS pode não ser a melhor ideia, já que a resposta provavelmente estará desatualizada.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYsDI7BEGysR"
   },
   "source": [
    "# **Metodologia Aplicada**\n",
    "\n",
    "Baixamos os PDFs das documentações da AWS e armazenamos no S3, utilizamos a técnica RAG para construir um sistema que usa o contexto dos PDFs e da pergunta para gerar uma resposta baseada nos PDFs (mais detalhes sobre o processo estão descritos no Notebook).\n",
    "\n",
    "![Question](https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/b886384153a387218fe0fae175586aa8a061f86c/03_QuestionAnswering/images/Chatbot_lang.png)\n",
    "\n",
    "Basicamente, convertemos os PDFs em pedaços de texto (chunks), vetorizamos esses chunks e armazenamos em um VectorStore. Com isso, cada pergunta é vetorizada para buscarmos os chunks com contexto mais similar a pergunta (busca vetorial por similaridade) e usamos os chunks mais relevantes como contexto no prompt do LLM para que esse LLM responda a pergunta usando o contexto dos PDFs.\n",
    "\n",
    "### Técnicas utilizadas\n",
    "- O armazenamento dos arquivos PDF foi feito utilizando o Amazon S3.\n",
    "\n",
    "- A geração de texto para interpretação da pergunta e elaboração da resposta foi feita utilizando Amazon Bedrock, rodando o modelo Cloude V2 da Anthropic.\n",
    "\n",
    "- Para agregar o conhecimento específico dos arquivos PDF no processo de geração da resposta utilizamos a técnica Retreival Augmented Generation (RAG).\n",
    "\n",
    "- A conversão dos textos em vetores foi feita usando o modelo Amazon Titan Embeddings G1 - Text, do Amazon Bedrock\n",
    "\n",
    "- O armazenamento dos vetores foi feito com a implementação FAISS\n",
    "\n",
    "- Para orquestrar esses processos, utilizamos a biblioteca [Langchain](https://python.langchain.com/docs/get_started/introduction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZrYw9xfHkf8"
   },
   "source": [
    "# **Resultados**\n",
    "\n",
    "O sistema consegue gerar respostas a partir de uma pergunta usando apenas o contexto dos PDFs.\n",
    "\n",
    "Porém, converter os PDFs em chunks e criar os vetores destes chunks é muito demorado (para 4 arquivos levou 40 minutos), por isso ter um sistema produtivo que usa todas as documentações da AWS teria um desafio de aumentar a eficiência do processo para conseguir contar com mais documentações em um tempo razoável de processamento.\n",
    "\n",
    "Enquanto as perguntas são relacionadas aos serviços que estão na base de conhecimento, o sistema responde de forma coerente, porém, perguntas fora dos 4 serviços que usamos como base de conhecimento ou recebem um retorno que não há informações suficientes ou utiliza de menções desses serviços na documentações dos que estão na base (ex: Amazon Kinesis, que aparece da documentação do DynamoDB).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpgoIA2KIWL4"
   },
   "source": [
    "# **Conclusão**\n",
    "\n",
    "O projeto alcançou os resultados esperados, uma vez que foi capaz de responder perguntas com base no conhecimento das documentações da AWS. Porém, ele poderia ser melhorado com um sistema mais eficiênte de transformar o conteúdo das documentações em vetor para conseguirmos colocar mais arquivos de documentação e, com isso, o sistema conseguir responder sobre outros serviços da AWS.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIqtrfXMI1Ad"
   },
   "source": [
    "# **Referências**\n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-workshop/tree/b886384153a387218fe0fae175586aa8a061f86c/03_QuestionAnswering\n",
    "\n",
    "\n",
    "https://github.com/aws-samples/llm-apps-workshop/blob/main/blogs/rag/data_ingestion_to_vectordb.ipynb\n",
    "\n",
    "https://github.com/aws-samples/rag-using-langchain-amazon-bedrock-and-opensearch/blob/main/load-data-to-opensearch.py\n",
    "\n",
    "https://catalog.us-east-1.prod.workshops.aws/workshops/a4bdb007-5600-4368-81c5-ff5b4154f518/en-US/50-qa/52-rag-qa\n",
    "\n",
    "https://js.langchain.com/docs/get_started/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWlnQXeaFJ7f"
   },
   "source": [
    "# **Vídeo**\n",
    "\n",
    "Explicação completa do trabalho: https://www.youtube.com/watch?v=2q9uzgVZ0Sw (20 min)\n",
    "\n",
    "Demonstração do funcionamento: https://www.youtube.com/watch?v=Xfnm9IsjTeo (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsPNTNhqNNEC"
   },
   "source": [
    "# Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_ls59TzW0Cn"
   },
   "source": [
    "### Introdução\n",
    "A ideia do Projeto é utilizar técnicas de Inteligência Artificial para construir um sistema de perguntas e respostas usando uma base de conhecimento de arquivos PDF que contém documentação de serviços da AWS.\n",
    "\n",
    "\n",
    "### Técnicas utilizadas\n",
    "- O armazenamento dos arquivos PDF foi feito utilizando o Amazon S3.\n",
    "\n",
    "- A geração de texto para interpretação da pergunta e elaboração da resposta foi feita utilizando Amazon Bedrock, rodando o modelo Cloude V2 da Anthropic.\n",
    "\n",
    "- Para agregar o conhecimento específico dos arquivos PDF no processo de geração da resposta utilizamos a técnica Retreival Augmented Generation (RAG).\n",
    "\n",
    "- Para orquestrar esses processos, utilizamos a biblioteca [Langchain](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQhSxn2932kn"
   },
   "source": [
    "### Import das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qEI9biOdMvA"
   },
   "source": [
    "Instalar as bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8205,
     "status": "ok",
     "timestamp": 1700576702916,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Rh71QgR8dvsw",
    "outputId": "e0748b9b-e599-4667-eca1-9dfb75719869"
   },
   "outputs": [],
   "source": [
    "!pip install -q gwpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34918,
     "status": "ok",
     "timestamp": 1700576737826,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Ecri9IXFXA0i",
    "outputId": "9d725edd-cba1-4fcb-e66d-b645589a23f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 660 µs (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install ipython-autotime\n",
    "!pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n",
    "!pip install --quiet \\\n",
    "    langchain==0.0.309 \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\"\n",
    "\n",
    "!pip install --quiet \\\n",
    "    pandas_datareader  \\\n",
    "    langchain_experimental \\\n",
    "\n",
    "%load_ext autotime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1700576737827,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Ke-h1SM1W0Cp",
    "outputId": "d38426fd-a756-425c-b1d6-579e7f48623d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.9 ms (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zvSZyucuH46"
   },
   "source": [
    "### Funções Úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1700576738194,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "lnIuvKQSuHr6",
    "outputId": "0553026b-e111-4aeb-a3e7-0a346add8931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 133 ms (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\"\"\"Helper utilities for working with Amazon Bedrock from Python notebooks\"\"\"\n",
    "# Python Built-Ins:\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1700576738195,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "uEFRj8K70qzf",
    "outputId": "d05b7100-ebe6-451c-a2ff-d5dd4552fbc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 762 µs (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1700576738195,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "XLEI6UQsRilG",
    "outputId": "1bb5ccb2-2fb9-4a1e-de91-9d9b16ca071e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 962 µs (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def create_vector_embedding_with_bedrock(text, name, bedrock_client):\n",
    "    payload = {\"inputText\": f\"{text}\"}\n",
    "    body = json.dumps(payload)\n",
    "    modelId = \"amazon.titan-embed-text-v1\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return {\"_index\": name, \"text\": text, \"vector_field\": embedding}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oanizWRRutyJ"
   },
   "source": [
    "### Setup do client\n",
    "\n",
    "Configura o cliente do Bedrock com as credenciais de acesso da AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1700576738195,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "nE21nKOxW0Cq",
    "outputId": "2ce81320-7893-4e39-fb9c-bf40c9b55892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "time: 120 ms (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"  # E.g. \"us-east-1\"\n",
    "\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQnlUAxdW0Cr"
   },
   "source": [
    "## Setup do Langchain\n",
    "\n",
    "Instanciar o modelo de LLM e de Embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1700576739363,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "hbBUJAyOW0Cs",
    "outputId": "0fb3170b-59c4-44fe-9a39-eba5d60d07b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 617 ms (started: 2024-02-27 17:32:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={'max_tokens_to_sample':200})\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlm244ldW0Cs"
   },
   "source": [
    "## Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzLzJNh6QpkK"
   },
   "source": [
    "#### Download dos arquivos do S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4544,
     "status": "ok",
     "timestamp": 1700576743903,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "JmEHMfry3klB",
    "outputId": "56b25d0b-fb60-42e5-90e3-17b71232c429"
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mciencia-de-dados-rag-t2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m bucket_objects \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContents\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m arquivos_pdf \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m bucket_objects:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the ListObjects operation: Access Denied"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 656 ms (started: 2024-02-27 17:32:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "from pprint import pprint\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "bucket_name =\"ciencia-de-dados-rag-t2\"\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_objects = s3.list_objects(Bucket=bucket_name)['Contents']\n",
    "\n",
    "arquivos_pdf = []\n",
    "\n",
    "for obj in bucket_objects:\n",
    "  obj_name = obj['Key']\n",
    "  arquivos_pdf.append(obj_name)\n",
    "  if not os.path.isfile(f'./data/{obj_name}'):\n",
    "    print(f'Baixando o arquivo {obj_name}...')\n",
    "    s3.download_file(bucket_name, obj_name, f'./data/{obj_name}')\n",
    "  else:\n",
    "    print(f'Arquivo {obj_name} já está baixado!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hros593hQsv0"
   },
   "source": [
    "### Vetorização dos arquivos\n",
    "\n",
    "Antes de responder as perguntas, os arquivos PDF precisam ser processados e armazenados em um banco de dados de vetor. Para isso, quebramos os arquivos em chunks e depois transformamos os textos em vetores, para armazenar essas informações em um banco de vetores (FAISS).\n",
    "\n",
    "![Embeddings](https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/b886384153a387218fe0fae175586aa8a061f86c/03_QuestionAnswering/images/Embeddings_lang.png)\n",
    "\n",
    "\n",
    "\n",
    "Basicamente, o processo é:\n",
    "- Carregar os documentos do S3*\n",
    "- Processar e separar em chunks menores\n",
    "- Criar um vetor numérico que represente cada Chunk usando o modelo Amazon Bedrock Titan Embeddings\n",
    "- Criar um index com esses chunks e seus correspondentes embeddings\n",
    "\n",
    "\n",
    "*Após download, vamos carregar todos os arquivos do diretório usando o [DirectoryLoader from PyPDF available under LangChain](https://python.langchain.com/en/latest/reference/modules/document_loaders.html) e quebrar em chunks menores usando o [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html).\n",
    "\n",
    "O tamanho dos Chunks é uma decisão importante, pois ele deve ser grande suficiente para conter os contextos dos textos, porém pequeno suficiente para caber dentro do Prompt do LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 461573,
     "status": "ok",
     "timestamp": 1700577205472,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "XrG0C6PzW0Cs",
    "outputId": "632561f7-f634-44f9-85f2-01e21ddee3b9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1700583742776,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "dVnL4xgcEACK",
    "outputId": "f2fe8bb2-2bdc-4c31-dfb3-06d3ec0d70cb"
   },
   "outputs": [],
   "source": [
    "#Exemplo do conteúdo de um chunk\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvvjWGm5W0Ct"
   },
   "source": [
    "Exemplo de como ficaria um chunk vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1700577205908,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Du9J4hrIW0Ct",
    "outputId": "3025f2af-d4f8-4b9d-d380-93c16e05ac92"
   },
   "outputs": [],
   "source": [
    "sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Chunk: \", docs[0].page_content, \"\\n\\n\")\n",
    "\n",
    "print(\"Amostra do chunk vetorizado: \", sample_embedding, \"\\n\\n\")\n",
    "print(\"Tamanho do embeddings: \", sample_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7n20Ff1W0Ct"
   },
   "source": [
    "Como mostrado acima, vamos calcular os vetores para todos os chunks e armazenar em um storage de vetores.\n",
    "\n",
    "Isso será feito usando a implementação [FAISS](https://github.com/facebookresearch/faiss) do  [LangChain](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html)  que utiliza do modelo de embeddings (no caso o Bedrock Titan Embeddings) e a lista de chunks para criar o armazenamento de vetores.\n",
    "\n",
    "Após isso, criamos um Index para possibilitar as consultas semânticas em cima do storage de vetor. Para isso, usamos a implementação do [VectorStoreIndexWrapper](https://python.langchain.com/en/latest/modules/indexes/getting_started.html#one-line-index-creation).\n",
    "\n",
    "O FAISS é um vector store que fica armazenado em memória, para uma solução definitiva, poderíamos usar um serviço como OpenSearch para persistência dos vetores.\n",
    "\n",
    "**⚠️⚠️⚠️ NOTA: Como os PDF possuem muitas páginas, essa célula pode levar algumas horas para rodar ⚠️⚠️⚠️**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1700577212949,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Y0PGR5Zz01U4",
    "outputId": "1ba4a614-b138-4346-f3de-1d87b21c2358"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13963,
     "status": "ok",
     "timestamp": 1700584637599,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "c0_cTTdAW0Ct",
    "outputId": "1c238bda-e6cd-435c-9225-03d394af40d3"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs[0:100],\n",
    "    bedrock_embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTIkvwf0W0Ct"
   },
   "source": [
    "## Respondendo as perguntas\n",
    "\n",
    "Agora que já possuimos os arquivos vetorizados, podemos começar a fazer perguntas\n",
    "\n",
    "#### Fluxo da solução\n",
    "![Question](https://raw.githubusercontent.com/aws-samples/amazon-bedrock-workshop/b886384153a387218fe0fae175586aa8a061f86c/03_QuestionAnswering/images/Chatbot_lang.png)\n",
    "\n",
    "O fluxo é:\n",
    "- Criar um vetor com a pergunta\n",
    "- Fazer uma busca no storage de vetor com o vetor da pergunta (busca semântica)\n",
    "- Pega os top N chunks mais relevantes\n",
    "- Adicionar esses chunks como parte do contexto do prompt da LLM\n",
    "- Mandar o prompt para o LLM\n",
    "- Recebe a resposta da LLM baseada nos documentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPcKHEtYhwvW"
   },
   "source": [
    "## Exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brr9Q6VRh8El"
   },
   "source": [
    "#### Pergunta sobre traffic mirroring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1700582806814,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "6I5ccX76EC6l",
    "outputId": "a1e7b802-aba2-4105-d583-a8520d49e0dd"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"How does traffic mirroring works?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FgvTNHOW0Cu"
   },
   "source": [
    "Primeiro passo é criar um vetor da pergunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700582807832,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "VRqAmOXLW0Cu",
    "outputId": "04081df7-faca-4822-a80a-f48e45253bda"
   },
   "outputs": [],
   "source": [
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "np.array(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM8RCGMyW0Cu"
   },
   "source": [
    "Com esse vetor, buscamos pelos chunks relevantes (vetores mais parecidos com o da pergunta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700582807832,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "5JMtFmP_W0Cu",
    "outputId": "fc5aeafd-7b79-44d3-fd2e-e05b170dbccd"
   },
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print_ww(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojbxEMiXW0Cu"
   },
   "source": [
    "Agora basta usar a LLM para gerar uma resposta com base no contexto dos chunks mais parecidos com a pergunta.\n",
    "\n",
    "Para isso, usamos a implementação do Prompt template do Langchain.\n",
    "\n",
    "O Langchain possui uma abstração em cima do VectorStore que faz todo o processo acima e depois consulta a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700582807832,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "LIINpUJFW0Cu",
    "outputId": "1eb540ee-7fe5-4202-d60e-6902f9a536df"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "prompt_template = \"\"\"\n",
    "\n",
    "Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "<context>\n",
    "{context}\n",
    "</context\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6192,
     "status": "ok",
     "timestamp": 1700582814021,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "etE3we4qW0Cu",
    "outputId": "1200dc96-37b6-43f2-c5d5-b24ca265373a"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1700582814021,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "WrEzZm6X25XW",
    "outputId": "3267bf25-a319-4072-88c3-b52e0bb7a6cc"
   },
   "outputs": [],
   "source": [
    "print(answer['query'])\n",
    "print(\"\\n\\n\")\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC81su-hjICQ"
   },
   "source": [
    "### Pergunta sobre Aurora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13175,
     "status": "ok",
     "timestamp": 1700582827182,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "HdDhA70tW0Cv",
    "outputId": "069b32da-1e17-4b03-d77f-12f8068d47ee"
   },
   "outputs": [],
   "source": [
    "query = \"What is the best database engine for Aurora?\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1700582827182,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "xh36mI7rwX2L",
    "outputId": "0936050d-f86d-4bd1-cae5-35c098a6e54b"
   },
   "outputs": [],
   "source": [
    "print(answer['query'])\n",
    "print(\"\\n\\n\")\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU6OZRP2jSfC"
   },
   "source": [
    "### Pergunta sobre DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12788,
     "status": "ok",
     "timestamp": 1700582839945,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Yl_j6VA-jVNC",
    "outputId": "2fee773d-1819-4db5-dce4-5766d53fde2f"
   },
   "outputs": [],
   "source": [
    "query= \"How much does DynamoDB cost?\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1700582839945,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "eU4riRHz23s3",
    "outputId": "6546b6ee-c659-47ba-dc1a-de9e6b064f66"
   },
   "outputs": [],
   "source": [
    "print(answer['query'])\n",
    "print(\"\\n\\n\")\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_bZXgxJL-wl"
   },
   "source": [
    "### Pergunta sobre Amazon HealthLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7993,
     "status": "ok",
     "timestamp": 1700582847925,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "j8kbrOYdMB6q",
    "outputId": "17ec51c6-1561-4349-b89b-a8647c0e72e0"
   },
   "outputs": [],
   "source": [
    "query= \"How does Amazon HealthLake works?\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1700582847926,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "8oJE6j8UMCyB",
    "outputId": "54347e47-f844-4213-d07a-9baeff4d6a09"
   },
   "outputs": [],
   "source": [
    "print(answer['query'])\n",
    "print(\"\\n\\n\")\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fd0JPInMGux"
   },
   "source": [
    "### Pergunta sobre Amazon Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9314,
     "status": "ok",
     "timestamp": 1700582857216,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "Yyqima7vMLb2",
    "outputId": "64419db0-c941-43dd-d8af-070a38d49400"
   },
   "outputs": [],
   "source": [
    "query= \"How does Amazon Kinesis works?\"\n",
    "answer = qa({\"query\": query})\n",
    "print_ww(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1700582857216,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "e2v_hWXsMMFu",
    "outputId": "216352a5-2994-419f-80c8-68b7b0d9d266"
   },
   "outputs": [],
   "source": [
    "print(answer['query'])\n",
    "print(\"\\n\\n\")\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1700582857216,
     "user": {
      "displayName": "Bruno Vilardi",
      "userId": "08676047985655654406"
     },
     "user_tz": 180
    },
    "id": "mZqCsShcolf3",
    "outputId": "f1a3d41a-efcc-4787-c9a0-b0b2b64036b5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1Qh-QwyQ7Zmm0iSPG36-jWEDbjIG9HEzX",
     "timestamp": 1700574070892
    }
   ]
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
